import streamlit as st
from streamlit_webrtc import webrtc_streamer, WebRtcMode, VideoProcessorBase
from aiortc.contrib.media import MediaRecorder
import tempfile
import numpy as np
import matplotlib.pyplot as plt
import torch
import ffmpeg
import sys
import os
from moviepy import VideoFileClip
import re
import whisper
import opencc

# AffectGPT æ¨ç†ä¾èµ–
sys.path.append(os.path.join(os.path.dirname(__file__), "AffectGPT"))
from affectgpt_inference import AffectGPTInference

# RPPG æ¨ç†ä¾èµ–
from rppg.demo import analyze_heart_rate

# Qwen æ¨ç†ä¾èµ–
from qwen import *


gpu_id = 0

# Streamlit
st.set_page_config(page_title="ç»“åˆç”Ÿç†ä¿¡å·çš„å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«ç³»ç»Ÿ", layout="wide")
st.title("ğŸ¥ ç»“åˆç”Ÿç†ä¿¡å·çš„å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«ç³»ç»Ÿ")

# æŒä¹…ç¼“å­˜æ¨¡å‹
@st.cache_resource(show_spinner=True)
def load_model():
    # AffectGPT æ¨¡å‹åŠ è½½
    model = AffectGPTInference(
        cfg_path="/home/zhangzijie/Emotion-rPPG-AI-Web/AffectGPT/train_configs/mercaptionplus_outputhybird_bestsetup_bestfusion_frame_lz.yaml",
        ckpt_path="/home/zhangzijie/Emotion-rPPG-AI-Web/AffectGPT/models/AffectGPT/mercaptionplus_outputhybird_bestsetup_bestfusion_frame_lz/mercaptionplus_outputhybird_bestsetup_bestfusion_frame_lz_20250408110/checkpoint_000030_loss_0.751.pth",
        zeroshot=True,
        gpu_id=gpu_id
    )
    # Qwen æ¨¡å‹åŠ è½½ï¼Œgpu_memory_utilizationä¸å ç”¨æ˜¾å­˜ç›¸å…³ï¼Œç›®å‰æ€»å ç”¨çº¦66G
    llm, tokenizer, sampling_params = func_read_batch_calling_model(modelname="Qwen25", gpu_memory_utilization=0.6)
    # Whisper æ¨¡å‹åŠ è½½
    whisper_model = whisper.load_model("medium", f'cuda:{gpu_id}' if torch.cuda.is_available() else 'cpu')
    return model, llm, tokenizer, sampling_params, whisper_model

model, llm, tokenizer, sampling_params, whisper_model = load_model()
st.success("âœ… å¤§æ¨¡å‹å·²åŠ è½½")

# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
def init_session_state():
    defaults = {
        "uploaded_file": None,
        "result": None,
        "history": [],
        # æ§åˆ¶æŸ¥çœ‹å†å²è®°å½•å˜é‡
        "view_history": False,
    }
    for key, val in defaults.items():
        st.session_state.setdefault(key, val)


init_session_state()
# æ¬¢è¿è¯­
if not st.session_state.view_history:
    st.markdown("""
    è¯¥åº”ç”¨æ”¯æŒï¼š
    - ä¸Šä¼ æˆ–å½•åˆ¶è§†é¢‘ï¼Œè§†é¢‘ä¸­åº”æœ‰äººè„¸ï¼›
    - è‡ªåŠ¨æå–è§†é¢‘å¤šæ¨¡æ€ä¿¡æ¯ï¼ˆéŸ³é¢‘ã€å­—å¹•ç­‰ï¼‰ï¼›
    - ä½¿ç”¨rPPGè¯†åˆ«æŠ€æœ¯åˆ†æå¿ƒç‡å’Œå‘¼å¸ç‡ï¼›
    - ç»“åˆä»¥ä¸Šä¿¡æ¯ï¼Œä½¿ç”¨å¤§æ¨¡å‹è¯†åˆ«æƒ…æ„Ÿå…³é”®è¯å’Œææ€§ï¼›
    - ç»“åˆä»¥ä¸Šä¿¡æ¯ï¼Œåˆ†ææƒ…æ„Ÿè¯†åˆ«çš„åŸå› ï¼›
    """)
else:
    st.markdown(f"æ­£åœ¨æŸ¥çœ‹å†å²è®°å½•{st.session_state.view_history_index + 1}")

class Result:
    def __init__(self, video_path=None, subtitle_text="", audio_path=None,
                 ov=None, ov_chi=None, sentiment=None, describe=None,
                 rppg_hr=None, rppg_img=None):
        self.video_path = video_path
        self.subtitle_text = subtitle_text
        self.audio_path = audio_path
        self.ov = ov
        self.ov_chi = ov_chi
        self.sentiment = sentiment
        self.describe = describe
        self.rppg_hr = rppg_hr
        self.rppg_img = rppg_img
        
def add_history():
    if st.session_state.result != None:
        print("Adding history for video path: " + st.session_state.result.video_path + ", subtitle: " + st.session_state.result.subtitle_text)
        st.session_state.history.append(st.session_state.result)
    else:
        print("Result is None, not adding history.")

def get_audio_path():
    if st.session_state.result.audio_path == None:
        try:
            with st.spinner("æ­£åœ¨æå–éŸ³é¢‘..."):
                video_clip = VideoFileClip(st.session_state.result.video_path)
                temp_audio = tempfile.NamedTemporaryFile(delete=False, suffix=".wav")
                video_clip.audio.write_audiofile(temp_audio.name, codec='pcm_s16le')
                video_clip.close()
                st.session_state.result.audio_path = temp_audio.name
            st.success("âœ… éŸ³é¢‘æå–æˆåŠŸ")
        except Exception as e:
            st.error("éŸ³é¢‘æå–å¤±è´¥ï¼Œå¯èƒ½æ˜¯æºè§†é¢‘ä¸­æ— éŸ³é¢‘ä¿¡æ¯ã€‚")
            st.session_state.result.audio_path = None
    if st.session_state.result.audio_path:
        st.audio(st.session_state.result.audio_path)

def get_subtitle_text():
    if st.session_state.result.subtitle_text == "":
        if st.session_state.result.audio_path == None:
            st.error("æ— æ³•è¿›è¡Œè¯­éŸ³è¯†åˆ«ï¼Œç¼ºå°‘éŸ³é¢‘ä¿¡æ¯ã€‚")
            st.session_state.result.subtitle_text = ""
            return
        with st.spinner("æ­£åœ¨è¯†åˆ«éŸ³é¢‘..."):
            result = whisper_model.transcribe(st.session_state.result.audio_path, initial_prompt="æ¥ä¸‹æ¥æ˜¯ä¸€æ®µè§†é¢‘çš„å­—å¹•ã€‚Here are subtitles of a video.")
            print("Result of whisper:" + result['text'])
            st.session_state.result.subtitle_text = result['text']
            # converter = opencc.OpenCC("t2s.json")
            # subtitle_text = converter.convert(subtitle_text)
            st.success("âœ… å­—å¹•è¯†åˆ«æˆåŠŸ")
    st.success("å­—å¹•ï¼š" + st.session_state.result.subtitle_text)
    if len(st.session_state.result.subtitle_text) > 125:
        with st.spinner("å­—å¹•ä¿¡æ¯è¿‡é•¿ï¼Œè°ƒç”¨Qwenç®€åŒ–å­—å¹•å†…å®¹..."):
            st.session_state.result.subtitle_text = subtitle_summarize_qwen(tokenizer, llm, sampling_params, 
                                                        subtitle=st.session_state.result.subtitle_text)
        st.success("âœ… å­—å¹•ç®€åŒ–ç»“æœï¼š" + st.session_state.result.subtitle_text)

# å±•ç¤ºOpen-Vocabularyç»“æœ
def display_ov_result():
    raw = st.session_state.result.ov
    emotions = [e.strip() for e in raw.strip("[]").split(",") if e.strip()]

    if emotions:
        colors = ["#e63946", "#f4a261", "#2a9d8f", "#457b9d", "#6a4c93"]
        tags = " ".join(
            f"<span style='font-size:26px;font-weight:700;color:{colors[i%5]};margin-right:12px;'>{e}</span>"
            for i, e in enumerate(emotions)
        )
        st.markdown(f"<div style='margin:12px 0;'>{tags}</div>", unsafe_allow_html=True)
    else:
        st.write("æœªè¯†åˆ«å‡ºè‹±æ–‡æƒ…ç»ªå…³é”®è¯ã€‚")

def display_ov_result_chi():
    raw = st.session_state.result.ov_chi
    emotions = [e.strip() for e in re.split(r"[ã€,ï¼Œ]", raw.strip("[]")) if e.strip()]

    if emotions:
        colors = ["#e63946", "#f4a261", "#2a9d8f", "#457b9d", "#6a4c93"]
        tags = " ".join(
            f"<span style='font-size:26px;font-weight:700;color:{colors[i%5]};margin-right:12px;'>{e}</span>"
            for i, e in enumerate(emotions)
        )
        st.markdown(f"<div style='margin:12px 0;'>{tags}</div>", unsafe_allow_html=True)
    else:
        st.write("æœªè¯†åˆ«å‡ºä¸­æ–‡æƒ…ç»ªå…³é”®è¯ã€‚")

def get_emotion_result_ov():
    if st.session_state.result.ov == None:
        with st.spinner("æ­£åœ¨è¿›è¡Œæƒ…ç»ªè¯†åˆ«..."):
            try:
                result_ov = model.infer_emotion_ov(
                    video_path=st.session_state.result.video_path,
                    audio_path=st.session_state.result.audio_path,
                    subtitle=st.session_state.result.subtitle_text
                )
                print(result_ov)
                st.session_state.result.ov = reason_to_openset_qwen(tokenizer, llm, sampling_params, result_ov)
                st.session_state.result.ov_chi = reason_to_openset_qwen_chi(tokenizer, llm, sampling_params, result_ov)
            except Exception as e:
                st.error(f"æƒ…ç»ªè¯†åˆ«å‡ºé”™ï¼š{e}")
        st.success("âœ… æƒ…ç»ªè¯†åˆ«å®Œæˆ")
    st.subheader("æƒ…ç»ªå…³é”®è¯ï¼š")
    display_ov_result()
    display_ov_result_chi()

def get_emotion_result_sentiment():
    if st.session_state.result.sentiment is None:
        with st.spinner("æ­£åœ¨è¿›è¡Œæƒ…ç»ªææ€§è¯†åˆ«..."):
            polarity = float(reason_to_valence_qwen(tokenizer, llm, sampling_params, reason=st.session_state.result.ov))
            fig, ax = plt.subplots(figsize=(6, 1))
            gradient = np.linspace(0, 1, 500).reshape(1, -1)
            ax.imshow(gradient, extent=(-1, 1, 0.4, 0.6),
                    cmap="rainbow", aspect="auto")  # è“-ç™½-çº¢
            ax.hlines(0.5, -1, 1, color="black", linewidth=1)
            ax.plot(polarity, 0.5, "o", markersize=16, color="black")
            ax.set_xticks([-1, -0.5, 0, 0.5, 1])
            ax.set_yticks([])
            ax.text(polarity, 0.7, f"{polarity:.2f}", ha="center", fontsize=12)
            ax.text(-1, 0.25, "ğŸ˜¡ Negative", ha="center", fontsize=12, color="blue")
            ax.text(0, 0.25, "ğŸ˜ Neutral", ha="center", fontsize=12, color="gray")
            ax.text(1, 0.25, "ğŸ˜„ Positive", ha="center", fontsize=12, color="red")
            for spine in ax.spines.values():
                spine.set_visible(False)
            st.session_state.result.sentiment = fig
        st.success("âœ… æƒ…ç»ªææ€§è¯†åˆ«å®Œæˆ")
    st.subheader("æƒ…ç»ªææ€§ï¼š")
    col1, col2, col3 = st.columns([1, 2, 1])
    with col2:
        st.pyplot(st.session_state.result.sentiment, use_container_width=False)

def get_emotion_result_describe():
    if st.session_state.result.describe == None:
        with st.spinner("æ­£åœ¨åˆ†æ..."):
            try:
                result_describe = model.infer_emotion_describe(
                    video_path=st.session_state.result.video_path,
                    audio_path=st.session_state.result.audio_path,
                    subtitle=st.session_state.result.subtitle_text
                )
                print(result_describe)
                st.markdown(f"åˆæ­¥åˆ†æï¼š{result_describe}")
                st.session_state.result.describe = reason_merge_qwen(tokenizer, llm, sampling_params, 
                                                                reason=result_describe,
                                                                subtitle=st.session_state.result.subtitle_text,
                                                                hr=st.session_state.result.rppg_hr,
                                                                ov=st.session_state.result.ov)
                st.session_state.result.describe += "\n\n"
                st.session_state.result.describe += translate_eng2chi_qwen(tokenizer, llm, sampling_params, 
                                                                reason=st.session_state.result.describe)
            except Exception as e:
                st.error(f"æƒ…ç»ªåˆ†æå‡ºé”™ï¼š{e}")
        st.success("âœ… æƒ…ç»ªåˆ†æå®Œæˆ")
    st.subheader("æƒ…ç»ªåˆ†æç»“æœï¼š")
    st.markdown(st.session_state.result.describe)

def get_rppg():
    if st.session_state.result.rppg_hr == None:
        with st.spinner("æ­£åœ¨æ£€æµ‹..."):            
            st.session_state.result.rppg_hr, st.session_state.result.rppg_img = \
            analyze_heart_rate(st.session_state.result.video_path, gpu_id)
        st.success("âœ… ç”Ÿç†ä¿¡å·æ£€æµ‹å®Œæˆ")
    st.subheader("ç”Ÿç†ä¿¡å·æ£€æµ‹ç»“æœï¼š")
    st.metric("ä¼°è®¡å¿ƒç‡", f"{st.session_state.result.rppg_hr:.2f} bpm")
    st.image(st.session_state.result.rppg_img, caption="æ³¢å½¢å›¾", use_container_width=True)


if not st.session_state.view_history:
    # ä¸Šä¼ æˆ–æ‹æ‘„è§†é¢‘
    option = st.radio("é€‰æ‹©è¾“å…¥æ–¹å¼ï¼š", ["ä¸Šä¼ è§†é¢‘æ–‡ä»¶", "ä½¿ç”¨æ‘„åƒå¤´æ‹æ‘„"])
    if option == "ä¸Šä¼ è§†é¢‘æ–‡ä»¶":
        uploaded_file = st.file_uploader("è¯·ä¸Šä¼ è§†é¢‘æ–‡ä»¶", type=["mp4", "mov", "avi", "mkv"])
        if uploaded_file != None and st.session_state.uploaded_file != uploaded_file:
            st.session_state.uploaded_file = uploaded_file
            temp_file = tempfile.NamedTemporaryFile(delete=False)
            temp_file.write(uploaded_file.read())
            if uploaded_file.type != "video/mp4":
                video_path = tempfile.NamedTemporaryFile(delete=False, suffix=".mp4").name
                ffmpeg.input(temp_file.name).output(video_path, vcodec='libx264', acodec='aac').run(overwrite_output=True)
            else:
                video_path = temp_file.name
            add_history()
            st.session_state.result = Result(video_path=video_path)
    elif option == "ä½¿ç”¨æ‘„åƒå¤´æ‹æ‘„":
        st.markdown("å½•åˆ¶è¯´æ˜ï¼šç‚¹å‡»ä¸‹æ–¹çš„â€œSTARTâ€æŒ‰é’®ï¼Œå…è®¸è®¿é—®æ‘„åƒå¤´åå³å¼€å§‹å½•åˆ¶è§†é¢‘ã€‚å½•åˆ¶å®Œæˆåç‚¹å‡»â€STOPâ€œï¼Œå¾…åŠ è½½å®Œæˆåå†ç‚¹å‡»â€œå®Œæˆå½•åˆ¶â€æŒ‰é’®ï¼Œä»¥è·å–å½•åˆ¶çš„è§†é¢‘ã€‚")
        def recorder_factory() -> MediaRecorder:
            return MediaRecorder('/tmp/record.mp4' , format="mp4")
        # å¯åŠ¨ WebRTC ä»¥å½•åˆ¶
        webrtc_streamer(
            key="record_only",
            mode=WebRtcMode.SENDRECV,
            media_stream_constraints={"video": True, "audio": True},  # å¯ç”¨éŸ³è§†é¢‘
            in_recorder_factory=recorder_factory,
        )
        if st.button("å®Œæˆå½•åˆ¶"):
            temp_file = tempfile.NamedTemporaryFile(delete=False)
            with open('/tmp/record.mp4', "rb") as record:
                temp_file.seek(0)
                temp_file.truncate()
                temp_file.write(record.read())
            video_path=temp_file.name
            add_history()
            st.session_state.result = Result(video_path=video_path)
    # å±•ç¤ºè§†é¢‘
if st.session_state.result != None and st.session_state.result.video_path != None:
    st.video(st.session_state.result.video_path)


if not st.session_state.view_history:
    # å­—å¹•è¾“å…¥
    st.subheader("ğŸ’¬ è§†é¢‘é‡Œçš„äººè¯´äº†ä»€ä¹ˆï¼Ÿ")
    subtitle_text = st.text_area("è¯·è¾“å…¥å­—å¹•ï¼ˆå¯é€‰ï¼‰", placeholder="è‹¥ä¸è¾“å…¥ï¼Œå°†è‡ªåŠ¨è¿›è¡Œè¯­éŸ³è¯†åˆ«ã€‚æ³¨æ„ï¼šé”™è¯¯çš„è¾“å…¥å°†æ˜¾è‘—å½±å“è¯†åˆ«ç»“æœã€‚", height=100)
    # ç”¨æˆ·è¾“å…¥äº†å­—å¹•
    if st.session_state.result != None:
        if subtitle_text != "":
            print("User input subtitle: " + subtitle_text)
            st.session_state.user_subtitle_text = subtitle_text
            # å­˜å‚¨çš„å­—å¹•ä¿¡æ¯ä¸è¾“å…¥ä¸ä¸€è‡´ï¼Œåˆ™æ›´æ–°
            if st.session_state.result.subtitle_text != st.session_state.user_subtitle_text:
                add_history()
                st.session_state.result = Result(
                    video_path=st.session_state.result.video_path,
                    audio_path=st.session_state.result.audio_path,
                    subtitle_text=st.session_state.user_subtitle_text
                )
        else:
            # æœ¬æ¬¡æœªè¾“å…¥å­—å¹•ï¼Œä½†ç”¨æˆ·ä¸Šä¸€æ¬¡è¾“å…¥äº†å­—å¹•
            if hasattr(st.session_state, "user_subtitle_text"):
                del st.session_state.user_subtitle_text
                add_history()
                st.session_state.result = Result(
                    video_path=st.session_state.result.video_path,
                    audio_path=st.session_state.result.audio_path,
                )
            # æœªè¾“å…¥å­—å¹•ï¼Œä¸”ä¸Šä¸€æ¬¡ä¹Ÿæœªè¾“å…¥å­—å¹•ï¼Œåˆ™ç»§ç»­ä½¿ç”¨è¯­éŸ³è¯†åˆ«ç»“æœ        


if st.session_state.result != None and st.session_state.result.video_path != None:
    if st.button("æ£€æµ‹ç”Ÿç†ä¿¡å·"):
        get_rppg()
    if st.button("åˆ†ææƒ…ç»ªå…³é”®è¯"):
        get_audio_path()
        get_subtitle_text()
        get_emotion_result_ov()
        get_emotion_result_sentiment()
    if st.button("æè¿°æƒ…ç»ª"):
        get_audio_path()
        get_subtitle_text()
        get_emotion_result_describe()
else:
    st.info("è¯·å…ˆä¸Šä¼ æˆ–æ‹æ‘„ä¸€æ¡è§†é¢‘ã€‚")

# ç‚¹å‡»ä¾§è¾¹æ æŒ‰é’®çš„è¡Œä¸ºï¼šè‹¥æ­£åœ¨æŸ¥çœ‹å†å²è®°å½•ï¼Œåˆ™ä¿å­˜å½“å‰è®°å½•ï¼›å¦åˆ™æ–°å¢å†å²è®°å½•
def click_sidebar_button():
    if (not st.session_state.view_history):
        add_history()
    elif st.session_state.view_history:
        st.session_state.history[st.session_state.view_history_index] = st.session_state.result

# å†å²è®°å½•æ 
with st.sidebar:
    if st.button("æ–°å»ºåˆ†æ"):
        click_sidebar_button()
        st.session_state.view_history = False
        st.session_state.result = None
        st.rerun()

    st.title("ğŸ’¬ å†å²è®°å½•")
    if(len(st.session_state.history) == 0):
        st.write("æš‚æ— å†å²è®°å½•ã€‚")
    for i in range(len(st.session_state.history)-1, -1, -1):
        if st.button(f"è®°å½• {i+1} " + st.session_state.history[i].subtitle_text):
            click_sidebar_button()
            st.session_state.view_history = True
            st.session_state.view_history_index = i
            st.session_state.result = st.session_state.history[i]
            st.rerun()
    