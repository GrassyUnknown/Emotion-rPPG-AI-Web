import streamlit as st
from streamlit_webrtc import webrtc_streamer, WebRtcMode, VideoProcessorBase
from aiortc.contrib.media import MediaRecorder
import tempfile
import numpy as np
import time
import torch
import random
import sys
import os
from moviepy import VideoFileClip
import re
import whisper
import opencc

# AffectGPT æ¨ç†ä¾èµ–
sys.path.append(os.path.join(os.path.dirname(__file__), "AffectGPT"))
from affectgpt_inference import AffectGPTInference

# RPPG æ¨ç†ä¾èµ–
from rppg.demo import analyze_heart_rate

# Qwen æ¨ç†ä¾èµ–
from qwen import *

gpu_id = 0

# Streamlit
st.set_page_config(page_title="æƒ…æ„Ÿè¯†åˆ«ä¸å¿ƒç‡æ£€æµ‹", layout="wide")
st.title("ğŸ¥ å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«ä¸å¿ƒç‡æ£€æµ‹ç³»ç»Ÿ")

# æŒä¹…ç¼“å­˜æ¨¡å‹
@st.cache_resource(show_spinner=True)
def load_model():
    # AffectGPT æ¨¡å‹åŠ è½½
    model = AffectGPTInference(
        cfg_path="/home/zhangzijie/Emotion-rPPG-AI-Web/AffectGPT/train_configs/mercaptionplus_outputhybird_bestsetup_bestfusion_frame_lz.yaml",
        ckpt_path="/home/zhangzijie/Emotion-rPPG-AI-Web/AffectGPT/models/AffectGPT/mercaptionplus_outputhybird_bestsetup_bestfusion_frame_lz/mercaptionplus_outputhybird_bestsetup_bestfusion_frame_lz_20250408110/checkpoint_000030_loss_0.751.pth",
        zeroshot=True,
        gpu_id=gpu_id
    )
    # Qwen æ¨¡å‹åŠ è½½
    llm, tokenizer, sampling_params = func_read_batch_calling_model(modelname="Qwen25")
    # Whisper æ¨¡å‹åŠ è½½
    whisper_model = whisper.load_model("medium", f'cuda:{gpu_id}' if torch.cuda.is_available() else 'cpu')
    return model, llm, tokenizer, sampling_params, whisper_model

try:
    model, llm, tokenizer, sampling_params, whisper_model = load_model()
    st.success("âœ… å¤§æ¨¡å‹å·²åŠ è½½")
except Exception as e:
    st.error(f"âš ï¸ æ¨¡å‹åŠ è½½å¤±è´¥ï¼š{e}")
    model = None

st.markdown("""
è¯¥åº”ç”¨æ”¯æŒï¼š
- ä¸Šä¼ æˆ–å½•åˆ¶è§†é¢‘ï¼›
- è‡ªåŠ¨æå–è§†é¢‘éŸ³é¢‘ï¼›
- ä½¿ç”¨ AffectGPT æ¨¡å‹åˆ†ææƒ…æ„ŸçŠ¶æ€ï¼›
- ä½¿ç”¨ Contrast-Phys åˆ†æå¿ƒç‡çŠ¶æ€ï¼›
""")

# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
def init_session_state():
    defaults = {
        "uploaded_file": None,
        "video_path": "",
        "subtitle_text": "",
        "audio_path": "",
        "result_ov": "",
        "result_ov_chi": "",
        "result_describe": "",
    }
    for key, val in defaults.items():
        st.session_state.setdefault(key, val)
init_session_state()

def get_audio_path():
    if st.session_state.audio_path == "":
        with st.spinner("æ­£åœ¨æå–éŸ³é¢‘..."):
            video_clip = VideoFileClip(st.session_state.video_path)
            temp_audio = tempfile.NamedTemporaryFile(delete=False, suffix=".wav")
            video_clip.audio.write_audiofile(temp_audio.name, codec='pcm_s16le')
            video_clip.close()
            st.session_state.audio_path = temp_audio.name
    if st.session_state.audio_path:
        st.success("âœ… éŸ³é¢‘æå–æˆåŠŸ")
        st.audio(st.session_state.audio_path)

def get_subtitle_text():
    if st.session_state.subtitle_text == "":
        with st.spinner("æ­£åœ¨è¯†åˆ«éŸ³é¢‘..."):
            result = whisper_model.transcribe(st.session_state.audio_path, initial_prompt="æ¥ä¸‹æ¥æ˜¯ä¸€æ®µè§†é¢‘çš„å­—å¹•ã€‚Here are subtitles of a video.")
            print("Result of whisper:" + result['text'])
            st.session_state.subtitle_text = result['text']
            # converter = opencc.OpenCC("t2s.json")
            # subtitle_text = converter.convert(subtitle_text)
    st.success("âœ… å­—å¹•ï¼š" + st.session_state.subtitle_text)
    if len(st.session_state.subtitle_text) > 125:
        with st.spinner("å­—å¹•ä¿¡æ¯è¿‡é•¿ï¼Œè°ƒç”¨Qwenç®€åŒ–å­—å¹•å†…å®¹..."):
            st.session_state.subtitle_text = subtitle_summarize_qwen(tokenizer, llm, sampling_params, 
                                                        subtitle=st.session_state.subtitle_text)
        st.success("âœ… å­—å¹•ç®€åŒ–ç»“æœï¼š" + st.session_state.subtitle_text)

# å±•ç¤ºOpen-Vocabularyç»“æœ
def display_ov_result():
    raw = st.session_state.result_ov
    emotions = [e.strip() for e in raw.strip("[]").split(",") if e.strip()]

    if emotions:
        colors = ["#e63946", "#f4a261", "#2a9d8f", "#457b9d", "#6a4c93"]
        tags = " ".join(
            f"<span style='font-size:26px;font-weight:700;color:{colors[i%5]};margin-right:12px;'>{e}</span>"
            for i, e in enumerate(emotions)
        )
        st.markdown(f"<div style='margin:12px 0;'>{tags}</div>", unsafe_allow_html=True)
    else:
        st.write("æœªè¯†åˆ«å‡ºè‹±æ–‡æƒ…ç»ªå…³é”®è¯ã€‚")

def display_ov_result_chi():
    raw = st.session_state.result_ov_chi
    emotions = [e.strip() for e in re.split(r"[ã€,ï¼Œ]", raw.strip("[]")) if e.strip()]

    if emotions:
        colors = ["#e63946", "#f4a261", "#2a9d8f", "#457b9d", "#6a4c93"]
        tags = " ".join(
            f"<span style='font-size:26px;font-weight:700;color:{colors[i%5]};margin-right:12px;'>{e}</span>"
            for i, e in enumerate(emotions)
        )
        st.markdown(f"<div style='margin:12px 0;'>{tags}</div>", unsafe_allow_html=True)
    else:
        st.write("æœªè¯†åˆ«å‡ºä¸­æ–‡æƒ…ç»ªå…³é”®è¯ã€‚")

def get_emotion_result_ov():
    if st.session_state.result_ov == "":
        with st.spinner("æ­£åœ¨è¿›è¡Œæƒ…ç»ªè¯†åˆ«..."):
            try:
                result_ov = model.infer_emotion_ov(
                    video_path=st.session_state.video_path,
                    audio_path=st.session_state.audio_path,
                    subtitle=st.session_state.subtitle_text
                )
                print(result_ov)
                st.session_state.result_ov = reason_to_openset_qwen(tokenizer, llm, sampling_params, result_ov)
                st.session_state.result_ov_chi = reason_to_openset_qwen_chi(tokenizer, llm, sampling_params, result_ov)
            except Exception as e:
                st.error(f"æƒ…ç»ªè¯†åˆ«å‡ºé”™ï¼š{e}")
    st.success("âœ… æƒ…ç»ªè¯†åˆ«å®Œæˆ")
    st.subheader("æƒ…ç»ªè¯†åˆ«ç»“æœï¼š")
    display_ov_result()
    display_ov_result_chi()

def get_emotion_result_describe():
    if st.session_state.result_describe == "":
        with st.spinner("æ­£åœ¨è¿›è¡Œæƒ…ç»ªè¯†åˆ«..."):
            try:
                result_describe = model.infer_emotion_describe(
                    video_path=st.session_state.video_path,
                    audio_path=st.session_state.audio_path,
                    subtitle=st.session_state.subtitle_text
                )
                print(result_describe)
                st.session_state.result_describe = reason_merge_qwen(tokenizer, llm, sampling_params, 
                                                                reason=result_describe,
                                                                subtitle=st.session_state.subtitle_text)
                st.session_state.result_describe += "\n\n"
                st.session_state.result_describe += translate_eng2chi_qwen(tokenizer, llm, sampling_params, 
                                                                reason=st.session_state.result_describe)
            except Exception as e:
                st.error(f"æƒ…ç»ªè¯†åˆ«å‡ºé”™ï¼š{e}")
    st.success("âœ… æƒ…ç»ªè¯†åˆ«å®Œæˆ")
    st.subheader("æƒ…ç»ªè¯†åˆ«ç»“æœï¼š")
    st.markdown(st.session_state.result_describe)

# å½“ä¸Šä¼ æ–°è§†é¢‘æ—¶ï¼Œæ¸…ç©ºç»“æœ
def clear_session_state_with_new_video():
    st.session_state.subtitle_text = ""
    st.session_state.audio_path = ""
    st.session_state.result_ov = ""
    st.session_state.result_describe = ""

# ä¸Šä¼ æˆ–æ‹æ‘„è§†é¢‘
option = st.radio("é€‰æ‹©è¾“å…¥æ–¹å¼ï¼š", ["ä¸Šä¼ è§†é¢‘æ–‡ä»¶", "ä½¿ç”¨æ‘„åƒå¤´æ‹æ‘„"])
if option == "ä¸Šä¼ è§†é¢‘æ–‡ä»¶":
    uploaded_file = st.file_uploader("è¯·ä¸Šä¼ è§†é¢‘æ–‡ä»¶ï¼ˆmp4 / mov / aviï¼‰", type=["mp4", "mov", "avi"])
    if uploaded_file != None and st.session_state.uploaded_file != uploaded_file:
        st.session_state.uploaded_file = uploaded_file
        temp_file = tempfile.NamedTemporaryFile(delete=False)
        temp_file.write(uploaded_file.read())
        st.session_state.video_path = temp_file.name
        clear_session_state_with_new_video()
elif option == "ä½¿ç”¨æ‘„åƒå¤´æ‹æ‘„":
    def recorder_factory() -> MediaRecorder:
        return MediaRecorder('/tmp/record.mp4' , format="mp4")
    # å¯åŠ¨ WebRTC ä»¥å½•åˆ¶
    webrtc_streamer(
        key="record_only",
        mode=WebRtcMode.SENDRECV,
        media_stream_constraints={"video": True, "audio": True},  # å¯ç”¨éŸ³è§†é¢‘
        in_recorder_factory=recorder_factory,
    )
    try:
        print(st.session_state.video)
    except AttributeError as e:
        temp_file = tempfile.NamedTemporaryFile(suffix=".mp4", delete=False)
        st.session_state.video = temp_file
        print(st.session_state.video)
    if st.button("å®Œæˆå½•åˆ¶"):
        with open('/tmp/record.mp4', "rb") as record:
            st.session_state.video.seek(0)
            st.session_state.video.truncate()
            st.session_state.video.write(record.read())
        st.session_state.video_path=st.session_state.video.name
        clear_session_state_with_new_video()
# å±•ç¤ºè§†é¢‘
if st.session_state.video_path != "":
    st.video(st.session_state.video_path)


# å­—å¹•è¾“å…¥
st.subheader("ğŸ’¬ è§†é¢‘é‡Œçš„äººè¯´äº†ä»€ä¹ˆï¼Ÿ")
subtitle_text = st.text_area("è¯·è¾“å…¥å­—å¹•ï¼ˆå¯é€‰ï¼‰", placeholder="è‹¥ä¸è¾“å…¥ï¼Œå°†è‡ªåŠ¨è¿›è¡Œè¯­éŸ³è¯†åˆ«ã€‚æ³¨æ„ï¼šé”™è¯¯çš„è¾“å…¥å°†æ˜¾è‘—å½±å“è¯†åˆ«ç»“æœã€‚", height=100)
# ç”¨æˆ·è¾“å…¥äº†å­—å¹•
if subtitle_text != "":
    print("User input subtitle: " + subtitle_text)
    st.session_state.user_subtitle_text = subtitle_text
    # å­˜å‚¨çš„å­—å¹•ä¿¡æ¯ä¸è¾“å…¥ä¸ä¸€è‡´ï¼Œåˆ™æ›´æ–°
    if st.session_state.subtitle_text != st.session_state.user_subtitle_text:
        st.session_state.subtitle_text = st.session_state.user_subtitle_text
        st.session_state.result_ov = ""
        st.session_state.result_describe = ""
else:
    # æœ¬æ¬¡æœªè¾“å…¥å­—å¹•ï¼Œä½†ç”¨æˆ·ä¸Šä¸€æ¬¡è¾“å…¥äº†å­—å¹•
    if hasattr(st.session_state, "user_subtitle_text"):
        del st.session_state.user_subtitle_text
        st.session_state.subtitle_text = ""
        st.session_state.result_ov = ""
        st.session_state.result_describe = ""
    # æœªè¾“å…¥å­—å¹•ï¼Œä¸”ä¸Šä¸€æ¬¡ä¹Ÿæœªè¾“å…¥å­—å¹•ï¼Œåˆ™ç»§ç»­ä½¿ç”¨è¯­éŸ³è¯†åˆ«ç»“æœ        


if st.session_state.video_path != "":
    if st.button("åˆ†ææƒ…ç»ªå…³é”®è¯"):
        get_audio_path()
        get_subtitle_text()
        get_emotion_result_ov()
    if st.button("æè¿°æƒ…ç»ª"):
        get_audio_path()
        get_subtitle_text()
        get_emotion_result_describe()
    if st.button("æ£€æµ‹å¿ƒç‡"):
        with st.spinner("æ­£åœ¨æ£€æµ‹å¿ƒç‡..."):            
            hr, img = analyze_heart_rate(st.session_state.video_path, gpu_id)
            st.success("âœ… å¿ƒç‡æ£€æµ‹å®Œæˆ")
            st.subheader("å¿ƒç‡æ£€æµ‹ç»“æœï¼š")
            st.metric("ä¼°è®¡å¿ƒç‡", f"{hr:.2f} bpm")
            st.image(img, caption="rPPG æ³¢å½¢ä¸åŠŸç‡è°±", use_container_width=True)
else:
    st.info("è¯·å…ˆä¸Šä¼ æˆ–æ‹æ‘„ä¸€æ¡è§†é¢‘ã€‚")

#TODO: å¢åŠ å†å²è®°å½•
with st.sidebar:
    st.title("ğŸ’¬ å†å²è®°å½•")
    st.markdown("_åŠŸèƒ½å¼€å‘ä¸­ï¼Œæ•¬è¯·æœŸå¾…ï¼_")