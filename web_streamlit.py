import streamlit as st
from streamlit_webrtc import webrtc_streamer, WebRtcMode, VideoProcessorBase
from aiortc.contrib.media import MediaRecorder
import av
import cv2
import tempfile
import numpy as np
import time
import torch
import random
import sys
import os
from moviepy import VideoFileClip
import re
import whisper
import opencc

# ------------------------------
# AffectGPT æ¨ç†ä¾èµ–
# ------------------------------
sys.path.append(os.path.join(os.path.dirname(__file__), "AffectGPT"))
from affectgpt_inference import AffectGPTInference

# ------------------------------
# RPPG æ¨ç†ä¾èµ–
# ------------------------------
from rppg.demo import analyze_heart_rate

gpu_id = 0

# =======================================
# å·¥å…·å‡½æ•°ï¼šéŸ³é¢‘æå–
# =======================================
def extract_audio_from_video(video_path):
    """ä»è§†é¢‘ä¸­æå–éŸ³é¢‘ä¸º WAV æ–‡ä»¶"""
    try:
        video_clip = VideoFileClip(video_path)
        temp_audio = tempfile.NamedTemporaryFile(delete=False, suffix=".wav")
        video_clip.audio.write_audiofile(temp_audio.name, codec='pcm_s16le')
        video_clip.close()
        return temp_audio.name
    except Exception as e:
        st.error(f"éŸ³é¢‘æå–å¤±è´¥ï¼š{e}")
        return None


# =======================================
# Streamlit é¡µé¢é€»è¾‘
# =======================================
st.set_page_config(page_title="æƒ…æ„Ÿè¯†åˆ«ä¸å¿ƒç‡æ£€æµ‹", layout="wide")
st.title("ğŸ¥ å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«ä¸å¿ƒç‡æ£€æµ‹ç³»ç»Ÿ")

# æŒä¹…ç¼“å­˜æ¨¡å‹
@st.cache_resource(show_spinner=True)
def load_affectgpt_model():
    model = AffectGPTInference(
        cfg_path="/home/zhangzijie/Emotion-rPPG-AI-Web/AffectGPT/train_configs/mercaptionplus_outputhybird_bestsetup_bestfusion_frame_lz.yaml",
        ckpt_path="/home/zhangzijie/Emotion-rPPG-AI-Web/AffectGPT/models/AffectGPT/mercaptionplus_outputhybird_bestsetup_bestfusion_frame_lz/mercaptionplus_outputhybird_bestsetup_bestfusion_frame_lz_20250408110/checkpoint_000030_loss_0.751.pth",
        zeroshot=True,
        gpu_id=gpu_id
    )
    return model

try:
    model = load_affectgpt_model()
    st.success("âœ… æ¨¡å‹å·²åŠ è½½")
except Exception as e:
    st.error(f"âš ï¸ æ¨¡å‹åŠ è½½å¤±è´¥ï¼š{e}")
    model = None

def display_emotion_result(result_text: str):

    # 1ï¸âƒ£ æå–æƒ…æ„Ÿå…³é”®è¯éƒ¨åˆ†
    match = re.search(r"emotional state is ([^.]+)[.]", result_text, re.IGNORECASE)
    emotions = []

    if match:
        emotion_part = match.group(1)
        # ç»Ÿä¸€åˆ†éš”ç¬¦
        emotion_part = re.sub(r"\band\b|/|&|;", ",", emotion_part)
        # æ‹†åˆ†æƒ…æ„Ÿè¯
        emotions = [e.strip(" ,") for e in emotion_part.split(",") if len(e.strip()) > 0]
        # å»é‡ & é¦–å­—æ¯å°å†™
        emotions = list(dict.fromkeys([e.lower() for e in emotions]))

    # 3ï¸âƒ£ è¾“å‡ºå±•ç¤º
    st.markdown("### ğŸ§  æ£€æµ‹åˆ°çš„æƒ…æ„ŸçŠ¶æ€")

    if emotions:
        colors = ["#e63946", "#f4a261", "#2a9d8f", "#457b9d", "#6a4c93"]
        emotion_tags = " ".join(
            [
                f"<span style='font-size:26px; font-weight:700; color:{colors[i % len(colors)]}; margin-right:10px;'>{e}</span>"
                for i, e in enumerate(emotions)
            ]
        )
        st.markdown(f"<div style='margin:10px 0;'>{emotion_tags}</div>", unsafe_allow_html=True)
    else:
        st.markdown("_æœªè¯†åˆ«åˆ°æ˜æ˜¾çš„æƒ…æ„Ÿå…³é”®è¯ã€‚_")


st.markdown("""
è¯¥åº”ç”¨æ”¯æŒï¼š
- ä¸Šä¼ æˆ–å½•åˆ¶è§†é¢‘ï¼›
- è‡ªåŠ¨æå–è§†é¢‘éŸ³é¢‘ï¼›
- ä½¿ç”¨ AffectGPT æ¨¡å‹åˆ†ææƒ…æ„ŸçŠ¶æ€ï¼›
- ä½¿ç”¨ Contrast-Phys åˆ†æå¿ƒç‡çŠ¶æ€ï¼›
""")

# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
def init_session_state():
    defaults = {
        "uploaded_file": None,
        "video_path": "",
        "subtitle_text": "",
        "audio_path": "",
        "result_ov": "",
        "result_describe": "",
    }
    for key, val in defaults.items():
        st.session_state.setdefault(key, val)

def get_audio_path():
    if st.session_state.audio_path == "":
        with st.spinner("æ­£åœ¨æå–éŸ³é¢‘..."):
            st.session_state.audio_path = extract_audio_from_video(st.session_state.video_path)
    if st.session_state.audio_path:
        st.success("âœ… éŸ³é¢‘æå–æˆåŠŸ")
        st.audio(st.session_state.audio_path)

def get_subtitle_text():
    if st.session_state.subtitle_text == "":
        with st.spinner("æ­£åœ¨è¯†åˆ«éŸ³é¢‘..."):
            whisper_model = whisper.load_model("small", f'cuda:{gpu_id}' if torch.cuda.is_available() else 'cpu')
            result = whisper_model.transcribe(st.session_state.audio_path, initial_prompt="æ¥ä¸‹æ¥æ˜¯ä¸€æ®µè§†é¢‘çš„å­—å¹•ã€‚Here are subtitles of a video.")
            print("Result of whisper:" + result['text'])
            st.session_state.subtitle_text = result['text']
            # converter = opencc.OpenCC("t2s.json")
            # subtitle_text = converter.convert(subtitle_text)
    st.success("âœ… è§†é¢‘é‡Œçš„äººè¯´ï¼š" + st.session_state.subtitle_text)

#TODO: æ¥å…¥Qwenå¹¶ä¿®æ”¹å±•ç¤ºå‡½æ•°
def get_emotion_result_ov():
    if st.session_state.result_ov == "":
        with st.spinner("æ­£åœ¨è¿›è¡Œæƒ…ç»ªè¯†åˆ«..."):
            try:
                st.session_state.result_ov = model.infer_emotion_ov(
                    video_path=st.session_state.video_path,
                    audio_path=st.session_state.audio_path,
                    subtitle=st.session_state.subtitle_text
                )
                print(st.session_state.result_ov)
            except Exception as e:
                st.error(f"æƒ…ç»ªè¯†åˆ«å‡ºé”™ï¼š{e}")
    st.success("âœ… æƒ…ç»ªè¯†åˆ«å®Œæˆ")
    st.subheader("æƒ…ç»ªè¯†åˆ«ç»“æœï¼š")
    display_emotion_result(st.session_state.result_ov)

def get_emotion_result_describe():
    if st.session_state.result_describe == "":
        with st.spinner("æ­£åœ¨è¿›è¡Œæƒ…ç»ªè¯†åˆ«..."):
            try:
                st.session_state.result_describe = model.infer_emotion_describe(
                    video_path=st.session_state.video_path,
                    audio_path=st.session_state.audio_path,
                    subtitle=st.session_state.subtitle_text
                )
                print(st.session_state.result_describe)
            except Exception as e:
                st.error(f"æƒ…ç»ªè¯†åˆ«å‡ºé”™ï¼š{e}")
    st.success("âœ… æƒ…ç»ªè¯†åˆ«å®Œæˆ")
    st.subheader("æƒ…ç»ªè¯†åˆ«ç»“æœï¼š")
    st.markdown(st.session_state.result_describe)

init_session_state()
# ä¸Šä¼ æˆ–æ‹æ‘„è§†é¢‘
option = st.radio("é€‰æ‹©è¾“å…¥æ–¹å¼ï¼š", ["ä¸Šä¼ è§†é¢‘æ–‡ä»¶", "ä½¿ç”¨æ‘„åƒå¤´æ‹æ‘„"])
if option == "ä¸Šä¼ è§†é¢‘æ–‡ä»¶":
    uploaded_file = st.file_uploader("è¯·ä¸Šä¼ è§†é¢‘æ–‡ä»¶ï¼ˆmp4 / mov / aviï¼‰", type=["mp4", "mov", "avi"])
    if uploaded_file != None and st.session_state.uploaded_file != uploaded_file:
        st.session_state.uploaded_file = uploaded_file
        temp_file = tempfile.NamedTemporaryFile(delete=False)
        temp_file.write(uploaded_file.read())
        st.session_state.video_path = temp_file.name
        st.session_state.subtitle_text = ""
        st.session_state.audio_path = ""
# ä½¿ç”¨æ‘„åƒå¤´æ‹æ‘„è§†é¢‘
elif option == "ä½¿ç”¨æ‘„åƒå¤´æ‹æ‘„":
    def recorder_factory() -> MediaRecorder:
        return MediaRecorder('/tmp/record.mp4' , format="mp4")

    # å¯åŠ¨ WebRTC
    webrtc_streamer(
        key="record_only",
        mode=WebRtcMode.SENDRECV,
        media_stream_constraints={"video": True, "audio": True},  # å¯ç”¨éŸ³è§†é¢‘
        in_recorder_factory=recorder_factory,
    )
    try:
        print(st.session_state.video)
    except AttributeError as e:
        temp_file = tempfile.NamedTemporaryFile(suffix=".mp4", delete=False)
        st.session_state.video = temp_file
        print(st.session_state.video)
    if st.button("å®Œæˆå½•åˆ¶"):
        with open('/tmp/record.mp4', "rb") as record:
            st.session_state.video.seek(0)
            st.session_state.video.truncate()
            st.session_state.video.write(record.read())
        st.session_state.video_path=st.session_state.video.name
        st.video(st.session_state.video_path)
        st.session_state.subtitle_text = ""
        st.session_state.audio_path = ""

if st.session_state.video_path != "":
    st.video(st.session_state.video_path)
# å­—å¹•è¾“å…¥
st.subheader("ğŸ’¬ è§†é¢‘é‡Œçš„äººè¯´äº†ä»€ä¹ˆï¼Ÿ")
subtitle_text = st.text_area("è¯·è¾“å…¥å­—å¹•ï¼ˆå¯é€‰ï¼‰", placeholder="è‹¥ä¸è¾“å…¥ï¼Œå°†è‡ªåŠ¨è¿›è¡Œè¯­éŸ³è¯†åˆ«ã€‚æ³¨æ„ï¼šé”™è¯¯çš„è¾“å…¥å°†æ˜¾è‘—å½±å“è¯†åˆ«ç»“æœã€‚", height=100)
# ç”¨æˆ·è¾“å…¥äº†å­—å¹•
if subtitle_text != "":
    print("User input subtitle: " + subtitle_text)
    st.session_state.user_subtitle_text = subtitle_text
    # å­˜å‚¨çš„å­—å¹•ä¿¡æ¯ä¸è¾“å…¥ä¸ä¸€è‡´ï¼Œåˆ™æ›´æ–°
    if st.session_state.subtitle_text != st.session_state.user_subtitle_text:
        st.session_state.subtitle_text = st.session_state.user_subtitle_text
        st.session_state.result_ov = ""
        st.session_state.result_describe = ""
else:
    # æœ¬æ¬¡æœªè¾“å…¥å­—å¹•ï¼Œä½†ç”¨æˆ·ä¸Šä¸€æ¬¡è¾“å…¥äº†å­—å¹•
    if hasattr(st.session_state, "user_subtitle_text"):
        del st.session_state.user_subtitle_text
        st.session_state.subtitle_text = ""
        st.session_state.result_ov = ""
        st.session_state.result_describe = ""
    # æœªè¾“å…¥å­—å¹•ï¼Œä¸”ä¸Šä¸€æ¬¡ä¹Ÿæœªè¾“å…¥å­—å¹•ï¼Œåˆ™ç»§ç»­ä½¿ç”¨è¯­éŸ³è¯†åˆ«ç»“æœ        


if st.session_state.video_path != "":
    if st.button("åˆ†ææƒ…ç»ªå…³é”®è¯"):
        get_audio_path()
        get_subtitle_text()
        get_emotion_result_ov()
    if st.button("æè¿°æƒ…ç»ª"):
        get_audio_path()
        get_subtitle_text()
        get_emotion_result_describe()
    if st.button("æ£€æµ‹å¿ƒç‡"):
        with st.spinner("æ­£åœ¨æ£€æµ‹å¿ƒç‡..."):            
            hr, img = analyze_heart_rate(st.session_state.video_path, gpu_id)
            st.success("âœ… å¿ƒç‡æ£€æµ‹å®Œæˆ")
            st.subheader("å¿ƒç‡æ£€æµ‹ç»“æœï¼š")
            st.metric("ä¼°è®¡å¿ƒç‡", f"{hr:.2f} bpm")
            st.image(img, caption="rPPG æ³¢å½¢ä¸åŠŸç‡è°±", use_container_width=True)
else:
    st.info("è¯·å…ˆä¸Šä¼ æˆ–æ‹æ‘„ä¸€æ¡è§†é¢‘ã€‚")

#TODO: å¢åŠ å†å²è®°å½•
with st.sidebar:
    st.title("ğŸ’¬ å†å²è®°å½•")
    st.markdown("_åŠŸèƒ½å¼€å‘ä¸­ï¼Œæ•¬è¯·æœŸå¾…ï¼_")