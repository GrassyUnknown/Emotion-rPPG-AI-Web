import streamlit as st
from streamlit_webrtc import webrtc_streamer, WebRtcMode, VideoProcessorBase
from aiortc.contrib.media import MediaRecorder
import av
import cv2
import tempfile
import numpy as np
import time
import torch
import random
import sys
import os
from moviepy import VideoFileClip
import re
import whisper
import opencc

# ------------------------------
# AffectGPT æ¨ç†ä¾èµ–
# ------------------------------
sys.path.append(os.path.join(os.path.dirname(__file__), "AffectGPT"))
from affectgpt_inference import AffectGPTInference

# ------------------------------
# RPPG æ¨ç†ä¾èµ–
# ------------------------------
from rppg.demo import analyze_heart_rate

gpu_id = 7

# =======================================
# å·¥å…·å‡½æ•°ï¼šéŸ³é¢‘æå–
# =======================================
def extract_audio_from_video(video_path):
    """ä»è§†é¢‘ä¸­æå–éŸ³é¢‘ä¸º WAV æ–‡ä»¶"""
    try:
        video_clip = VideoFileClip(video_path)
        temp_audio = tempfile.NamedTemporaryFile(delete=False, suffix=".wav")
        video_clip.audio.write_audiofile(temp_audio.name, codec='pcm_s16le')
        video_clip.close()
        return temp_audio.name
    except Exception as e:
        st.error(f"éŸ³é¢‘æå–å¤±è´¥ï¼š{e}")
        return None


# =======================================
# Streamlit é¡µé¢é€»è¾‘
# =======================================
st.set_page_config(page_title="æƒ…æ„Ÿè¯†åˆ«ä¸å¿ƒç‡æ£€æµ‹", layout="wide")
st.title("ğŸ¥ æƒ…æ„Ÿè¯†åˆ«ä¸å¿ƒç‡æ£€æµ‹ Demo")

# æŒä¹…ç¼“å­˜æ¨¡å‹
@st.cache_resource(show_spinner=True)
def load_affectgpt_model():
    model = AffectGPTInference(
        cfg_path="/home/zhangzijie/web/AffectGPT/train_configs/mercaptionplus_outputhybird_bestsetup_bestfusion_frame_lz.yaml",
        ckpt_path="/home/zhangzijie/ResearchFace/models/AffectGPT/mercaptionplus_outputhybird_bestsetup_bestfusion_frame_lz/mercaptionplus_outputhybird_bestsetup_bestfusion_frame_lz_20250408110/checkpoint_000030_loss_0.751.pth",
        zeroshot=True,
        gpu_id=gpu_id
    )
    return model

try:
    model = load_affectgpt_model()
    st.success("âœ… AffectGPT æ¨¡å‹å·²åŠ è½½")
except Exception as e:
    st.error(f"âš ï¸ æ¨¡å‹åŠ è½½å¤±è´¥ï¼š{e}")
    model = None

def display_emotion_result(result_text: str):

    # 1ï¸âƒ£ æå–æƒ…æ„Ÿå…³é”®è¯éƒ¨åˆ†
    match = re.search(r"emotional state is ([^.]+)[.]", result_text, re.IGNORECASE)
    emotions = []

    if match:
        emotion_part = match.group(1)
        # ç»Ÿä¸€åˆ†éš”ç¬¦
        emotion_part = re.sub(r"\band\b|/|&|;", ",", emotion_part)
        # æ‹†åˆ†æƒ…æ„Ÿè¯
        emotions = [e.strip(" ,") for e in emotion_part.split(",") if len(e.strip()) > 0]
        # å»é‡ & é¦–å­—æ¯å°å†™
        emotions = list(dict.fromkeys([e.lower() for e in emotions]))

    # 3ï¸âƒ£ è¾“å‡ºå±•ç¤º
    st.markdown("### ğŸ§  æ£€æµ‹åˆ°çš„æƒ…æ„ŸçŠ¶æ€")

    if emotions:
        colors = ["#e63946", "#f4a261", "#2a9d8f", "#457b9d", "#6a4c93"]
        emotion_tags = " ".join(
            [
                f"<span style='font-size:26px; font-weight:700; color:{colors[i % len(colors)]}; margin-right:10px;'>{e}</span>"
                for i, e in enumerate(emotions)
            ]
        )
        st.markdown(f"<div style='margin:10px 0;'>{emotion_tags}</div>", unsafe_allow_html=True)
    else:
        st.markdown("_æœªè¯†åˆ«åˆ°æ˜æ˜¾çš„æƒ…æ„Ÿå…³é”®è¯ã€‚_")


st.markdown("""
è¯¥åº”ç”¨æ”¯æŒï¼š
- ä¸Šä¼ æˆ–å½•åˆ¶è§†é¢‘ï¼›
- è‡ªåŠ¨æå–è§†é¢‘éŸ³é¢‘ï¼›
- ä½¿ç”¨ AffectGPT æ¨¡å‹åˆ†ææƒ…æ„ŸçŠ¶æ€ï¼›
- ä½¿ç”¨ Contrast-Phys åˆ†æå¿ƒç‡çŠ¶æ€ï¼›
""")

# ä¸Šä¼ æˆ–æ‹æ‘„è§†é¢‘
option = st.radio("é€‰æ‹©è¾“å…¥æ–¹å¼ï¼š", ["ä¸Šä¼ è§†é¢‘æ–‡ä»¶", "ä½¿ç”¨æ‘„åƒå¤´æ‹æ‘„"])
if option == "ä¸Šä¼ è§†é¢‘æ–‡ä»¶":
    uploaded_file = st.file_uploader("è¯·ä¸Šä¼ è§†é¢‘æ–‡ä»¶ï¼ˆmp4 / mov / aviï¼‰", type=["mp4", "mov", "avi"])
    if uploaded_file:
        temp_file = tempfile.NamedTemporaryFile(delete=False)
        temp_file.write(uploaded_file.read())
        st.session_state.video_path = temp_file.name
        st.video(st.session_state.video_path)
# ä½¿ç”¨æ‘„åƒå¤´æ‹æ‘„è§†é¢‘
elif option == "ä½¿ç”¨æ‘„åƒå¤´æ‹æ‘„":
    def recorder_factory() -> MediaRecorder:
        return MediaRecorder('/tmp/record.mp4' , format="mp4")

    # å¯åŠ¨ WebRTC
    webrtc_streamer(
        key="record_only",
        mode=WebRtcMode.SENDRECV,
        media_stream_constraints={"video": True, "audio": True},  # å¯ç”¨éŸ³è§†é¢‘
        in_recorder_factory=recorder_factory,
    )
    try:
        print(st.session_state.video)
    except AttributeError as e:
        temp_file = tempfile.NamedTemporaryFile(suffix=".mp4", delete=False)
        st.session_state.video = temp_file
        print(st.session_state.video)
    if st.button("å®Œæˆå½•åˆ¶"):
        with open('/tmp/record.mp4', "rb") as record:
            st.session_state.video.seek(0)
            st.session_state.video.truncate()
            st.session_state.video.write(record.read())
        st.session_state.video_path=st.session_state.video.name
        st.video(st.session_state.video_path)


# å­—å¹•è¾“å…¥
st.subheader("ğŸ’¬ è¾“å…¥è§†é¢‘å­—å¹•æˆ–è¯­éŸ³è¯†åˆ«æ–‡å­—")
subtitle_text = st.text_area("è¯·è¾“å…¥è§†é¢‘å¯¹åº”çš„æ–‡å­—å†…å®¹ï¼ˆå¯é€‰ï¼‰", placeholder="è‹¥ä¸è¾“å…¥ï¼Œå°†è‡ªåŠ¨è¯†åˆ«éŸ³é¢‘ã€‚è§†é¢‘è¿‡é•¿å¯èƒ½å¯¼è‡´è¯†åˆ«æ•ˆæœä¸ä½³ã€‚", height=100)


try:
    if st.button("åˆ†ææƒ…ç»ªå…³é”®è¯"):
        # -----------------------------
        # Step 1: æå–éŸ³é¢‘
        # -----------------------------
        with st.spinner("æ­£åœ¨æå–éŸ³é¢‘..."):
            audio_path = extract_audio_from_video(st.session_state.video_path)
            if audio_path:
                st.success("âœ… éŸ³é¢‘æå–æˆåŠŸ")
                st.audio(audio_path)
        # -----------------------------
        # Step 2: æå–æ–‡æœ¬
        # -----------------------------
        if subtitle_text == '':
            with st.spinner("æ­£åœ¨è¯†åˆ«éŸ³é¢‘..."):
                whisper_model = whisper.load_model("small", f'cuda:{gpu_id}' if torch.cuda.is_available() else 'cpu')
                result = whisper_model.transcribe(audio_path, initial_prompt="æ¥ä¸‹æ¥æ˜¯ä¸€æ®µè§†é¢‘çš„å­—å¹•ã€‚Here are subtitles of a video.")
                print("Result of whisper:" + result['text'])
                subtitle_text = result['text']
                # converter = opencc.OpenCC("t2s.json")
                # subtitle_text = converter.convert(subtitle_text)
                st.success("âœ… éŸ³é¢‘è¯†åˆ«æˆåŠŸï¼Œç»“æœä¸ºï¼š" + subtitle_text)
        # -----------------------------
        # Step 3: æƒ…ç»ªè¯†åˆ«
        # -----------------------------
        with st.spinner("æ­£åœ¨è¿›è¡Œæƒ…ç»ªè¯†åˆ«..."):
            try:
                if model:
                    result = model.infer_emotion_ov(
                        video_path=st.session_state.video_path,
                        audio_path=audio_path,
                        subtitle=subtitle_text
                    )
                    st.success("âœ… æƒ…ç»ªè¯†åˆ«å®Œæˆ")
                    st.subheader("æƒ…ç»ªè¯†åˆ«ç»“æœï¼š")
                    display_emotion_result(result)
                else:
                    st.error("æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè¯·ç¨åé‡æ–°åŠ è½½ç½‘é¡µ")

            except Exception as e:
                st.error(f"æƒ…ç»ªè¯†åˆ«å‡ºé”™ï¼š{e}")

    if st.button("æè¿°æƒ…ç»ª"):
        # -----------------------------
        # Step 1: æå–éŸ³é¢‘
        # -----------------------------
        with st.spinner("æ­£åœ¨æå–éŸ³é¢‘..."):
            audio_path = extract_audio_from_video(st.session_state.video_path)
            if audio_path:
                st.success("âœ… éŸ³é¢‘æå–æˆåŠŸ")
                st.audio(audio_path)
        # -----------------------------
        # Step 2: æå–æ–‡æœ¬
        # -----------------------------
        if subtitle_text == '':
            with st.spinner("æ­£åœ¨è¯†åˆ«éŸ³é¢‘..."):
                whisper_model = whisper.load_model("small", f'cuda:{gpu_id}' if torch.cuda.is_available() else 'cpu')
                result = whisper_model.transcribe(audio_path, initial_prompt="æ¥ä¸‹æ¥æ˜¯ä¸€æ®µè§†é¢‘çš„å­—å¹•ã€‚Here are subtitles of a video.")
                print("Result of whisper:" + result['text'])
                subtitle_text = result['text']
                # converter = opencc.OpenCC("t2s.json")
                # subtitle_text = converter.convert(subtitle_text)
                st.success("âœ… éŸ³é¢‘è¯†åˆ«æˆåŠŸï¼Œç»“æœä¸ºï¼š" + subtitle_text)
        # -----------------------------
        # Step 3: æƒ…ç»ªè¯†åˆ«
        # -----------------------------
        with st.spinner("æ­£åœ¨è¿›è¡Œæƒ…ç»ªè¯†åˆ«..."):
            try:
                if model:
                    result = model.infer_emotion_describe(
                        video_path=st.session_state.video_path,
                        audio_path=audio_path,
                        subtitle=subtitle_text
                    )
                    st.success("âœ… æƒ…ç»ªè¯†åˆ«å®Œæˆ")
                    st.subheader("æƒ…ç»ªè¯†åˆ«ç»“æœï¼š")
                    st.markdown(result)
                else:
                    st.error("æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè¯·ç¨åé‡æ–°åŠ è½½ç½‘é¡µ")

            except Exception as e:
                st.error(f"æƒ…ç»ªè¯†åˆ«å‡ºé”™ï¼š{e}")

    if st.button("æ£€æµ‹å¿ƒç‡"):
        # -----------------------------
        # å¿ƒç‡æ£€æµ‹ï¼ˆrPPGï¼‰
        # -----------------------------
        with st.spinner("æ­£åœ¨æ£€æµ‹å¿ƒç‡..."):
            
            hr, img = analyze_heart_rate(st.session_state.video_path, gpu_id)
            st.success("âœ… å¿ƒç‡æ£€æµ‹å®Œæˆ")
            st.subheader("å¿ƒç‡æ£€æµ‹ç»“æœï¼š")
            st.metric("ä¼°è®¡å¿ƒç‡", f"{hr:.2f} bpm")
            st.image(img, caption="rPPG æ³¢å½¢ä¸åŠŸç‡è°±", use_container_width=True)

except AttributeError:
    st.info("è¯·å…ˆä¸Šä¼ æˆ–æ‹æ‘„ä¸€ä¸ªè§†é¢‘ã€‚")
