import streamlit as st
from streamlit_webrtc import webrtc_streamer, WebRtcMode, VideoProcessorBase
from aiortc.contrib.media import MediaRecorder
import tempfile
import numpy as np
import time
import torch
import random
import sys
import os
from moviepy import VideoFileClip
import re
import whisper
import opencc

# AffectGPT æ¨ç†ä¾èµ–
sys.path.append(os.path.join(os.path.dirname(__file__), "AffectGPT"))
from affectgpt_inference import AffectGPTInference

# RPPG æ¨ç†ä¾èµ–
from rppg.demo import analyze_heart_rate

# Qwen æ¨ç†ä¾èµ–
from qwen import *

gpu_id = 0

# Streamlit
st.set_page_config(page_title="æƒ…æ„Ÿè¯†åˆ«ä¸å¿ƒç‡æ£€æµ‹", layout="wide")
st.title("ğŸ¥ å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«ä¸å¿ƒç‡æ£€æµ‹ç³»ç»Ÿ")

# æŒä¹…ç¼“å­˜æ¨¡å‹
@st.cache_resource(show_spinner=True)
def load_model():
    # AffectGPT æ¨¡å‹åŠ è½½
    model = AffectGPTInference(
        cfg_path="/home/zhangzijie/Emotion-rPPG-AI-Web/AffectGPT/train_configs/mercaptionplus_outputhybird_bestsetup_bestfusion_frame_lz.yaml",
        ckpt_path="/home/zhangzijie/Emotion-rPPG-AI-Web/AffectGPT/models/AffectGPT/mercaptionplus_outputhybird_bestsetup_bestfusion_frame_lz/mercaptionplus_outputhybird_bestsetup_bestfusion_frame_lz_20250408110/checkpoint_000030_loss_0.751.pth",
        zeroshot=True,
        gpu_id=gpu_id
    )
    # Qwen æ¨¡å‹åŠ è½½ï¼Œgpu_memory_utilizationä¸å ç”¨æ˜¾å­˜ç›¸å…³ï¼Œç›®å‰æ€»å ç”¨çº¦66G
    llm, tokenizer, sampling_params = func_read_batch_calling_model(modelname="Qwen25", gpu_memory_utilization=0.6)
    # Whisper æ¨¡å‹åŠ è½½
    whisper_model = whisper.load_model("medium", f'cuda:{gpu_id}' if torch.cuda.is_available() else 'cpu')
    return model, llm, tokenizer, sampling_params, whisper_model

model, llm, tokenizer, sampling_params, whisper_model = load_model()
st.success("âœ… å¤§æ¨¡å‹å·²åŠ è½½")

# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
def init_session_state():
    defaults = {
        "uploaded_file": None,
        "video_path": "",
        "subtitle_text": "",
        "audio_path": "",
        "result_ov": "",
        "result_ov_chi": "",
        "result_describe": "",
        "result_rppg_hr": "",
        "result_rppg_img": None,
        # ä»¥ä¸‹æ˜¯å†å²è®°å½•
        "video_path_history": [],
        "subtitle_text_history": [],
        "audio_path_history": [],
        "result_ov_history": [],
        "result_ov_chi_history": [],
        "result_describe_history": [],
        "result_rppg_hr_history": [],
        "result_rppg_img_history": [],
        # æ§åˆ¶æŸ¥çœ‹å†å²è®°å½•å˜é‡
        "view_history": False,
    }
    for key, val in defaults.items():
        st.session_state.setdefault(key, val)


init_session_state()
# æ¬¢è¿è¯­
if not st.session_state.view_history:
    st.markdown("""
    è¯¥åº”ç”¨æ”¯æŒï¼š
    - ä¸Šä¼ æˆ–å½•åˆ¶è§†é¢‘ï¼›
    - è‡ªåŠ¨æå–è§†é¢‘éŸ³é¢‘ï¼›
    - ä½¿ç”¨ AffectGPT æ¨¡å‹åˆ†ææƒ…æ„ŸçŠ¶æ€ï¼›
    - ä½¿ç”¨ Contrast-Phys åˆ†æå¿ƒç‡çŠ¶æ€ï¼›
    """)
else:
    st.markdown(f"æ­£åœ¨æŸ¥çœ‹å†å²è®°å½•{st.session_state.view_history_index + 1}")


def add_history():
    print("Adding history for video path: " + st.session_state.video_path + ", subtitle: " + st.session_state.subtitle_text)
    st.session_state.video_path_history.append(st.session_state.video_path)
    st.session_state.subtitle_text_history.append(st.session_state.subtitle_text)
    st.session_state.audio_path_history.append(st.session_state.audio_path)
    st.session_state.result_ov_history.append(st.session_state.result_ov)
    st.session_state.result_ov_chi_history.append(st.session_state.result_ov_chi)
    st.session_state.result_describe_history.append(st.session_state.result_describe)
    st.session_state.result_rppg_hr_history.append(st.session_state.result_rppg_hr)
    st.session_state.result_rppg_img_history.append(st.session_state.result_rppg_img)

def get_audio_path():
    if st.session_state.audio_path == "":
        try:
            with st.spinner("æ­£åœ¨æå–éŸ³é¢‘..."):
                video_clip = VideoFileClip(st.session_state.video_path)
                temp_audio = tempfile.NamedTemporaryFile(delete=False, suffix=".wav")
                video_clip.audio.write_audiofile(temp_audio.name, codec='pcm_s16le')
                video_clip.close()
                st.session_state.audio_path = temp_audio.name
        except Exception as e:
            st.error("éŸ³é¢‘æå–å¤±è´¥ï¼Œå¯èƒ½æ˜¯æºè§†é¢‘ä¸­æ— éŸ³é¢‘ä¿¡æ¯ã€‚")
            st.session_state.audio_path = ""
    if st.session_state.audio_path:
        st.success("âœ… éŸ³é¢‘æå–æˆåŠŸ")
        st.audio(st.session_state.audio_path)

def get_subtitle_text():
    if st.session_state.subtitle_text == "":
        if st.session_state.audio_path == "":
            st.error("æ— æ³•è¿›è¡Œè¯­éŸ³è¯†åˆ«ï¼Œç¼ºå°‘éŸ³é¢‘ä¿¡æ¯ã€‚")
            st.session_state.subtitle_text = ""
            return
        with st.spinner("æ­£åœ¨è¯†åˆ«éŸ³é¢‘..."):
            result = whisper_model.transcribe(st.session_state.audio_path, initial_prompt="æ¥ä¸‹æ¥æ˜¯ä¸€æ®µè§†é¢‘çš„å­—å¹•ã€‚Here are subtitles of a video.")
            print("Result of whisper:" + result['text'])
            st.session_state.subtitle_text = result['text']
            # converter = opencc.OpenCC("t2s.json")
            # subtitle_text = converter.convert(subtitle_text)
    st.success("âœ… å­—å¹•ï¼š" + st.session_state.subtitle_text)
    if len(st.session_state.subtitle_text) > 125:
        with st.spinner("å­—å¹•ä¿¡æ¯è¿‡é•¿ï¼Œè°ƒç”¨Qwenç®€åŒ–å­—å¹•å†…å®¹..."):
            st.session_state.subtitle_text = subtitle_summarize_qwen(tokenizer, llm, sampling_params, 
                                                        subtitle=st.session_state.subtitle_text)
        st.success("âœ… å­—å¹•ç®€åŒ–ç»“æœï¼š" + st.session_state.subtitle_text)

# å±•ç¤ºOpen-Vocabularyç»“æœ
def display_ov_result():
    raw = st.session_state.result_ov
    emotions = [e.strip() for e in raw.strip("[]").split(",") if e.strip()]

    if emotions:
        colors = ["#e63946", "#f4a261", "#2a9d8f", "#457b9d", "#6a4c93"]
        tags = " ".join(
            f"<span style='font-size:26px;font-weight:700;color:{colors[i%5]};margin-right:12px;'>{e}</span>"
            for i, e in enumerate(emotions)
        )
        st.markdown(f"<div style='margin:12px 0;'>{tags}</div>", unsafe_allow_html=True)
    else:
        st.write("æœªè¯†åˆ«å‡ºè‹±æ–‡æƒ…ç»ªå…³é”®è¯ã€‚")

def display_ov_result_chi():
    raw = st.session_state.result_ov_chi
    emotions = [e.strip() for e in re.split(r"[ã€,ï¼Œ]", raw.strip("[]")) if e.strip()]

    if emotions:
        colors = ["#e63946", "#f4a261", "#2a9d8f", "#457b9d", "#6a4c93"]
        tags = " ".join(
            f"<span style='font-size:26px;font-weight:700;color:{colors[i%5]};margin-right:12px;'>{e}</span>"
            for i, e in enumerate(emotions)
        )
        st.markdown(f"<div style='margin:12px 0;'>{tags}</div>", unsafe_allow_html=True)
    else:
        st.write("æœªè¯†åˆ«å‡ºä¸­æ–‡æƒ…ç»ªå…³é”®è¯ã€‚")

def get_emotion_result_ov():
    if st.session_state.result_ov == "":
        with st.spinner("æ­£åœ¨è¿›è¡Œæƒ…ç»ªè¯†åˆ«..."):
            try:
                result_ov = model.infer_emotion_ov(
                    video_path=st.session_state.video_path,
                    audio_path=st.session_state.audio_path,
                    subtitle=st.session_state.subtitle_text
                )
                print(result_ov)
                st.session_state.result_ov = reason_to_openset_qwen(tokenizer, llm, sampling_params, result_ov)
                st.session_state.result_ov_chi = reason_to_openset_qwen_chi(tokenizer, llm, sampling_params, result_ov)
            except Exception as e:
                st.error(f"æƒ…ç»ªè¯†åˆ«å‡ºé”™ï¼š{e}")
    st.success("âœ… æƒ…ç»ªè¯†åˆ«å®Œæˆ")
    st.subheader("æƒ…ç»ªè¯†åˆ«ç»“æœï¼š")
    display_ov_result()
    display_ov_result_chi()

def get_emotion_result_describe():
    if st.session_state.result_describe == "":
        with st.spinner("æ­£åœ¨è¿›è¡Œæƒ…ç»ªè¯†åˆ«..."):
            try:
                result_describe = model.infer_emotion_describe(
                    video_path=st.session_state.video_path,
                    audio_path=st.session_state.audio_path,
                    subtitle=st.session_state.subtitle_text
                )
                print(result_describe)
                st.markdown(f"åˆæ­¥è¯†åˆ«ç»“æœï¼ˆè‹±æ–‡ï¼‰ï¼š{result_describe}")
                st.session_state.result_describe = reason_merge_qwen(tokenizer, llm, sampling_params, 
                                                                reason=result_describe,
                                                                subtitle=st.session_state.subtitle_text)
                st.session_state.result_describe += "\n\n"
                st.session_state.result_describe += translate_eng2chi_qwen(tokenizer, llm, sampling_params, 
                                                                reason=st.session_state.result_describe)
            except Exception as e:
                st.error(f"æƒ…ç»ªè¯†åˆ«å‡ºé”™ï¼š{e}")
    st.success("âœ… æƒ…ç»ªè¯†åˆ«å®Œæˆ")
    st.subheader("æƒ…ç»ªè¯†åˆ«ç»“æœï¼š")
    st.markdown(st.session_state.result_describe)

def get_rppg():
    if st.session_state.result_rppg_hr == "":
        with st.spinner("æ­£åœ¨æ£€æµ‹å¿ƒç‡..."):            
            st.session_state.result_rppg_hr, st.session_state.result_rppg_img = \
            analyze_heart_rate(st.session_state.video_path, gpu_id)
    st.success("âœ… å¿ƒç‡æ£€æµ‹å®Œæˆ")
    st.subheader("å¿ƒç‡æ£€æµ‹ç»“æœï¼š")
    st.metric("ä¼°è®¡å¿ƒç‡", f"{st.session_state.result_rppg_hr:.2f} bpm")
    st.image(st.session_state.result_rppg_img, caption="rPPG æ³¢å½¢ä¸åŠŸç‡è°±", use_container_width=True)

# å½“ä¸Šä¼ æ–°è§†é¢‘æ—¶ï¼Œæ¸…ç©ºç»“æœ
def clear_session_state_with_new_video():
    st.session_state.subtitle_text = ""
    st.session_state.audio_path = ""
    st.session_state.result_ov = ""
    st.session_state.result_ov_chi = ""
    st.session_state.result_describe = ""
    st.session_state.result_rppg_hr = ""
    st.session_state.result_rppg_img = None

if not st.session_state.view_history:
    # ä¸Šä¼ æˆ–æ‹æ‘„è§†é¢‘
    option = st.radio("é€‰æ‹©è¾“å…¥æ–¹å¼ï¼š", ["ä¸Šä¼ è§†é¢‘æ–‡ä»¶", "ä½¿ç”¨æ‘„åƒå¤´æ‹æ‘„"])
    if option == "ä¸Šä¼ è§†é¢‘æ–‡ä»¶":
        uploaded_file = st.file_uploader("è¯·ä¸Šä¼ è§†é¢‘æ–‡ä»¶", type=["mp4", "mov", "avi"])
        if uploaded_file != None and st.session_state.uploaded_file != uploaded_file:
            if st.session_state.video_path != "":
                add_history()
            st.session_state.uploaded_file = uploaded_file
            temp_file = tempfile.NamedTemporaryFile(delete=False)
            temp_file.write(uploaded_file.read())
            st.session_state.video_path = temp_file.name
            clear_session_state_with_new_video()
    elif option == "ä½¿ç”¨æ‘„åƒå¤´æ‹æ‘„":
        st.markdown("å½•åˆ¶è¯´æ˜ï¼šç‚¹å‡»ä¸‹æ–¹çš„â€œSTARTâ€æŒ‰é’®ï¼Œå…è®¸è®¿é—®æ‘„åƒå¤´åå³å¼€å§‹å½•åˆ¶è§†é¢‘ã€‚å½•åˆ¶å®Œæˆåç‚¹å‡»â€STOPâ€œï¼Œå¾…åŠ è½½å®Œæˆåå†ç‚¹å‡»â€œå®Œæˆå½•åˆ¶â€æŒ‰é’®ï¼Œä»¥è·å–å½•åˆ¶çš„è§†é¢‘ã€‚")
        def recorder_factory() -> MediaRecorder:
            return MediaRecorder('/tmp/record.mp4' , format="mp4")
        # å¯åŠ¨ WebRTC ä»¥å½•åˆ¶
        webrtc_streamer(
            key="record_only",
            mode=WebRtcMode.SENDRECV,
            media_stream_constraints={"video": True, "audio": True},  # å¯ç”¨éŸ³è§†é¢‘
            in_recorder_factory=recorder_factory,
        )
        if st.button("å®Œæˆå½•åˆ¶"):
            if st.session_state.video_path != "":
                add_history()
            temp_file = tempfile.NamedTemporaryFile(delete=False)
            with open('/tmp/record.mp4', "rb") as record:
                temp_file.seek(0)
                temp_file.truncate()
                temp_file.write(record.read())
            st.session_state.video_path=temp_file.name
            clear_session_state_with_new_video()
    # å±•ç¤ºè§†é¢‘
if st.session_state.video_path != "":
    st.video(st.session_state.video_path)

if not st.session_state.view_history:
    # å­—å¹•è¾“å…¥
    st.subheader("ğŸ’¬ è§†é¢‘é‡Œçš„äººè¯´äº†ä»€ä¹ˆï¼Ÿ")
    subtitle_text = st.text_area("è¯·è¾“å…¥å­—å¹•ï¼ˆå¯é€‰ï¼‰", placeholder="è‹¥ä¸è¾“å…¥ï¼Œå°†è‡ªåŠ¨è¿›è¡Œè¯­éŸ³è¯†åˆ«ã€‚æ³¨æ„ï¼šé”™è¯¯çš„è¾“å…¥å°†æ˜¾è‘—å½±å“è¯†åˆ«ç»“æœã€‚", height=100)
    # ç”¨æˆ·è¾“å…¥äº†å­—å¹•
    if subtitle_text != "":
        print("User input subtitle: " + subtitle_text)
        st.session_state.user_subtitle_text = subtitle_text
        # å­˜å‚¨çš„å­—å¹•ä¿¡æ¯ä¸è¾“å…¥ä¸ä¸€è‡´ï¼Œåˆ™æ›´æ–°
        if st.session_state.subtitle_text != st.session_state.user_subtitle_text:
            st.session_state.subtitle_text = st.session_state.user_subtitle_text
            st.session_state.result_ov = ""
            st.session_state.result_describe = ""
    else:
        # æœ¬æ¬¡æœªè¾“å…¥å­—å¹•ï¼Œä½†ç”¨æˆ·ä¸Šä¸€æ¬¡è¾“å…¥äº†å­—å¹•
        if hasattr(st.session_state, "user_subtitle_text"):
            del st.session_state.user_subtitle_text
            st.session_state.subtitle_text = ""
            st.session_state.result_ov = ""
            st.session_state.result_describe = ""
        # æœªè¾“å…¥å­—å¹•ï¼Œä¸”ä¸Šä¸€æ¬¡ä¹Ÿæœªè¾“å…¥å­—å¹•ï¼Œåˆ™ç»§ç»­ä½¿ç”¨è¯­éŸ³è¯†åˆ«ç»“æœ        


if st.session_state.video_path != "":
    #TODO å°†å¿ƒç‡ä¿¡æ¯æ¥å…¥æƒ…æ„Ÿåˆ†æï¼Ÿ
    if st.button("åˆ†ææƒ…ç»ªå…³é”®è¯"):
        get_audio_path()
        get_subtitle_text()
        get_emotion_result_ov()
    if st.button("æè¿°æƒ…ç»ª"):
        get_audio_path()
        get_subtitle_text()
        get_emotion_result_describe()
    if st.button("æ£€æµ‹å¿ƒç‡"):
        get_rppg()

else:
    st.info("è¯·å…ˆä¸Šä¼ æˆ–æ‹æ‘„ä¸€æ¡è§†é¢‘ã€‚")

# ç‚¹å‡»ä¾§è¾¹æ æŒ‰é’®çš„è¡Œä¸ºï¼šè‹¥æ­£åœ¨æŸ¥çœ‹å†å²è®°å½•ï¼Œåˆ™ä¿å­˜å½“å‰è®°å½•ï¼›å¦åˆ™æ–°å¢å†å²è®°å½•
def click_sidebar_button():
    if (not st.session_state.view_history) and st.session_state.video_path != "":
        add_history()
    elif st.session_state.view_history:
        st.session_state.subtitle_text_history[st.session_state.view_history_index] = st.session_state.subtitle_text
        st.session_state.audio_path_history[st.session_state.view_history_index] = st.session_state.audio_path
        st.session_state.result_ov_history[st.session_state.view_history_index] = st.session_state.result_ov
        st.session_state.result_ov_chi_history[st.session_state.view_history_index] = st.session_state.result_ov_chi
        st.session_state.result_describe_history[st.session_state.view_history_index] = st.session_state.result_describe
        st.session_state.result_rppg_hr_history[st.session_state.view_history_index] = st.session_state.result_rppg_hr
        st.session_state.result_rppg_img_history[st.session_state.view_history_index] = st.session_state.result_rppg_img

# å†å²è®°å½•æ 
with st.sidebar:
    if st.button("æ–°å»ºåˆ†æ"):
        click_sidebar_button()
        st.session_state.view_history = False
        st.session_state.video_path = ""
        st.rerun()

    st.title("ğŸ’¬ å†å²è®°å½•")
    if(len(st.session_state.video_path_history) == 0):
        st.write("æš‚æ— å†å²è®°å½•ã€‚")
    for i in range(len(st.session_state.video_path_history)-1, -1, -1):
        if st.button(f"è®°å½• {i+1} " + st.session_state.subtitle_text_history[i]):
            click_sidebar_button()
            st.session_state.view_history = True
            st.session_state.view_history_index = i
            st.session_state.video_path = st.session_state.video_path_history[i]
            st.session_state.subtitle_text = st.session_state.subtitle_text_history[i]
            st.session_state.audio_path = st.session_state.audio_path_history[i]
            st.session_state.result_ov = st.session_state.result_ov_history[i]
            st.session_state.result_ov_chi = st.session_state.result_ov_chi_history[i]
            st.session_state.result_describe = st.session_state.result_describe_history[i]
            st.session_state.result_rppg_hr = st.session_state.result_rppg_hr_history[i]
            st.session_state.result_rppg_img = st.session_state.result_rppg_img_history[i]
            st.rerun()
    